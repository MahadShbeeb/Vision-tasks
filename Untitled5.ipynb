{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GetOldTweets3 as got\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#بحثنا بالفرب من السعودية بالتاريخ المحدد وبعدد 500 تغريدة وخزنا في ملف نصي\n",
    "tweetCriteria = got.manager.TweetCriteria().setNear(\"Saudi Arabia\")\\\n",
    "                                           .setSince(\"2019-11-19\")\\\n",
    "                                           .setUntil(\"2019-11-21\")\\\n",
    "                                           .setMaxTweets(500)\n",
    "\n",
    "tweet = got.manager.TweetManager.getTweets(tweetCriteria)[0:500]\n",
    "text=tweet\n",
    "f= open(\"my_tweets.txt\",\"w+\", encoding=\"utf-8\")\n",
    "for i in text:\n",
    "    f.write(f'{i.text}\\n')\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# حذفنا جميع الروابط بالاضافة الى روابط الصور\n",
    "def normalizeTweets(text):\n",
    "    result = re.sub(r'https\\S+',' ', text)\n",
    "    result = re.sub(r'pic\\S+',' ', result)\n",
    "    return(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#بدلنا كل ياء بالف مقصورة  \n",
    "#بداية حذفنا الاحرف المكررة وابقينا ع واحد منها وبدلنا كل تاء مربوطة بهاء وكل الف مهموزة او الف مع مدة بالف ممدودة\n",
    "def normalizeLetters(text):\n",
    "    result = re.sub(r'(.)\\1+', r'\\1', text)\n",
    "    result = re.sub(\"ة\", \"ه\", result)\n",
    "    result = re.sub(\"[أآ]\", \"ا\", result)\n",
    "    result = re.sub(\"ى\", \"ي\", result)\n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#اوجدنا كل هاشتاغ بالنص الممرر باخذ اشارة # مع الكتابة المرافقة \n",
    "def hashtags(text): \n",
    "    text=re.findall(r\"#(\\w+)\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#تخزيت الداتا من النص لمتحول data\n",
    "f= open(\"my_tweets.txt\",\"r\", encoding=\"utf-8\")\n",
    "data=f.readlines()\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ايران_برا_بلاد_العرب_حره', 5)\n",
      "('فيصل_سبعين_محب', 3)\n",
      "('الضمان', 3)\n",
      "('الجبهه_الشعبيه_لتحرير_راديو', 2)\n",
      "('iPhone', 2)\n",
      "('كلنا_الميسري_الجبواني', 2)\n",
      "('Aple', 2)\n",
      "('لاتغرد_في_هشتاق_سيء_ولو_نصيحه', 2)\n",
      "('موضي_قطعه_من_ذهب', 1)\n",
      "('اسقاط_القروض_ليوم_351', 1)\n"
     ]
    }
   ],
   "source": [
    "#استخدمنا ملف جديد لتخزين الداتا النظيفة \n",
    "#مررنا هذه الداتا لتابع الهاشتاغ لايجاد الهاشتاغات وخزناها في مصفوفة ثم ماوجدنا اكثر الهاشتاغات المكررة باستخدام تابع most_common()\n",
    "f= open(\"my_tweet_normalize.txt\",\"w+\", encoding=\"utf-8\")\n",
    "\n",
    "hashs=[]\n",
    "for i in data:\n",
    "    i=normalizeLetters(normalizeTweets(i))\n",
    "    f.write(f'{i}\\n')\n",
    "    if len(hashtags(i))!=0:\n",
    "        hashs.append(hashtags(i))\n",
    "arr=[]\n",
    "for i in hashs:\n",
    "    arr.append(i[0])\n",
    "\n",
    "fdist = nltk.FreqDist(arr)\n",
    "top = fdist.most_common(10)\n",
    "for i in top:\n",
    "    print(i)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#مررنا النص واستدعينا التوابع لتنظيف الداتا المجمعة  ثم اخذناها ككلمات منفصلة ومررنا كثنائيات واوجدنا التكرارات \n",
    "f= open(\"my_tweet_Analysis.txt\",\"w+\", encoding=\"utf-8\")\n",
    "for i in data:\n",
    "    i=normalizeLetters(normalizeTweets(i))\n",
    "    words = nltk.word_tokenize(i)\n",
    "    text_bigrams = nltk.bigrams(words)\n",
    "    fdist = nltk.FreqDist(text_bigrams)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in text_bigrams:\n",
    "    print(f'{text_bigrams}\\n ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the last commen :[('يا', 'حجي'), ('حجي', 'الشق'), ('الشق', 'عود'), ('عود', '.'), ('.', 'الفريق'), ('الفريق', 'يضحك'), ('يضحك', 'بالنص'), ('بالنص', 'حتي'), ('حتي', 'رابيو'), ('رابيو', 'و')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    print(f'the last commen :{fdist.hapaxes()[:10]}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most commen: [(('يا', 'حجي'), 1), (('حجي', 'الشق'), 1), (('الشق', 'عود'), 1), (('عود', '.'), 1), (('.', 'الفريق'), 1), (('الفريق', 'يضحك'), 1), (('يضحك', 'بالنص'), 1), (('بالنص', 'حتي'), 1), (('حتي', 'رابيو'), 1), (('رابيو', 'و'), 1)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    print(f'The most commen: {fdist.most_common(10)}\\n') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
